{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 便利的优化器\n",
    "\n",
    "4.2节介绍了自动求导机制，求出梯度（params.grad）后，我们进行了手动更新：params = (params - learning_rate * params.grad).detach().requires_grad_()， 这里使用的是最常用的SGD梯度优化方法。\n",
    "\n",
    "Pytorch封装了多种多样的优化器供大家使用，无需手动写出优化的公式，只需要调用接口定义即可。来看一看吧~\n",
    "\n",
    "### 4.3.1 初识\n",
    "\n",
    "首先来看看pytorch提供了哪些优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASGD', 'Adadelta', 'Adagrad', 'Adam', 'Adamax', 'LBFGS', 'Optimizer', 'RMSprop', 'Rprop', 'SGD', 'SparseAdam', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'lr_scheduler']\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "print(dir(optim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个优化器都提供两个方法：\n",
    "- zero_grad: 将传给优化器的所有变量的梯度都置零\n",
    "- step： 更具优化器的优化策略来更新传入的变量的值\n",
    "\n",
    "仍然沿用之前的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0013, 0.0010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 构建模型函数\n",
    "def model(x, w, b):\n",
    "    return w*x + b\n",
    "\n",
    "# 构建损失函数\n",
    "def loss_fn(y_p, y):\n",
    "    squared_diffs = (y_p - y) ** 2\n",
    "    return squared_diffs.mean()\n",
    "\n",
    "# x与y\n",
    "x = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "y = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x = 0.1 * x\n",
    "\n",
    "# 初始化参数\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 构建优化器\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)  # 注意这里（新增步骤）\n",
    "\n",
    "# 前向计算\n",
    "y_p = model(x, *params)\n",
    "loss = loss_fn(y_p, y)\n",
    "\n",
    "# 后向计算\n",
    "optimizer.zero_grad()  # 将优化器中的所有变量的梯度都置零\n",
    "loss.backward()  # 反向计算\n",
    "optimizer.step()  # 更新参数\n",
    "\n",
    "print(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上过程中，我们没有手动更新params,而是通过optimizer.step()进行自动更新，打印出来的pramas也确实被更新了。\n",
    "\n",
    "### 4.3.2 完整迭代过程\n",
    "上面是进行了一次迭代，下面来看看完整的迭代过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss 39.538250\n",
      "Epoch 200, Loss 18.696901\n",
      "Epoch 300, Loss 12.456582\n",
      "Epoch 400, Loss 10.581183\n",
      "Epoch 500, Loss 10.017575\n",
      "Epoch 600, Loss 9.848182\n",
      "Epoch 700, Loss 9.797276\n",
      "Epoch 800, Loss 9.781981\n",
      "Epoch 900, Loss 9.777378\n",
      "Epoch 1000, Loss 9.776002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([17.9473, 32.9443], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_loop(n_epochs, optimizer, params, x, y):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        y_p = model(x, *params)\n",
    "        loss = loss_fn(y_p, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        \n",
    "    return params\n",
    "\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 1000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    x = x,\n",
    "    y = y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结：\n",
    "- torch.optim 提供多类优化器\n",
    "- 优化器有2个方法：zero_grad, step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
