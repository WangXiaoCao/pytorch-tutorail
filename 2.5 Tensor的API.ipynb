{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tensor的API\n",
    "Pytorch提供了很多tensor的API， 如若逐条去记忆想必自然不是程序员的学习方法，更不是AI从业者的学习方法，API是帮助我们快速开发的工具，绝不能成为学习的压力，因此相对有效的方法是“类”--> “查” -->“熟”。\n",
    "\n",
    "- “类”：对纷繁众多的API进行归类，先归大类，再在大类中归小类，对知识形成一个系统的认知；\n",
    "\n",
    "- “查”：当在实际的应用中忽然想实现某个功能的时候，在“类”的帮助下马上知道了是什么类别下的API能帮助实现，于是去官方文档中迅速查找到函数名、使用方法及例子；\n",
    "\n",
    "- “熟”：这是自然而然的过程，在不断高效“查”的过程中对“类”也逐渐熟悉，于是根本无需死记硬背，就对海量的API熟悉了。\n",
    "\n",
    "pytorch tensor API的官方文档地址：http://pytorch.org/docs\n",
    "\n",
    "文档中给API归了大类，但是每个大类罗列的API是根据首字母的顺序，也不方便大家快速认知，因此本章的小节中不但按照官方的思路对大类进行讲解，也根据笔者的思路对大类再做了更细致的归类，方便大家对API的认知。\n",
    "\n",
    "### 2.5.1 调用API的方式\n",
    "**方式一：操作后创建新的变量**\n",
    "\n",
    "方式一中也有两种不同的方式可以选用\n",
    "\n",
    "- （1）使用Torch module中的方法\n",
    "\n",
    "用上文提到过的转置方法来举例， 可以调用torch.transpose(), 将原tensor:a作为参数传入， 生成新的tensor:a_t。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(3, 2)\n",
    "a_t = torch.transpose(a, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （2）使用tensor object的中方法\n",
    "\n",
    "直接在tensor：a上调用transpose方法，结果同上，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a_t = a.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此两种方法可以互相替换使用，但注意目前（2）中tensor对象的方法数目相对较少，用（1）torch moduel中的功能更全面。\n",
    "\n",
    "**方式二：在原变量上做操作**\n",
    "\n",
    "对tensor调用带有下划线结尾的方法，即为对原tensor进行操作，无需赋值新的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 索引、切块\n",
    "#### （1）简单索引与切块\n",
    "对tensor的索引和对python中list的索引是一样的，只是tensor是多维数组，list是一维的。对list的索引不再赘述，下面看几个tensor索引的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])\n",
    "points[1:]  # 获取第2行开始的所有行 [[2.0, 1.0],[3.0, 5.0]]\n",
    "points[1:, :]  # 获取第2行开始的所有行的所有列，结果与上同 [[2.0, 1.0],[3.0, 5.0]]\n",
    "points[1:, 0]  # 获取第2行开始的所有行的第0列， [[2.0],[3.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）高级索引与切块\n",
    "**a.通过索引切割tensor: torch.index_select()  &  torch.take()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7528, 0.4361, 0.5605, 0.5077],\n",
      "        [0.0770, 0.9452, 0.0484, 0.6103],\n",
      "        [0.2585, 0.1833, 0.2185, 0.4698],\n",
      "        [0.1420, 0.5039, 0.0059, 0.0589]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4,6)\n",
    "b = torch.index_select(input=a, dim=1, index=torch.LongTensor([0,1,2,3]))\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对a, 在维度1上，切出索引在[0,3）之间的元素。\n",
    "注意：index必须是LongTensor类型的。\n",
    "\n",
    "torch.take()也是通过索引取数，只是将Input平铺成一维的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7528, 0.4361, 0.5605, 0.5077])\n"
     ]
    }
   ],
   "source": [
    "b = torch.take(input=a, index=torch.LongTensor([0,1,2,3]))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.通过掩码切割tensor:torch.masked_select()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4115, 0.2656, 0.0737, 0.0494])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.ByteTensor([[0,0,1,0],[1,1,0,1]])\n",
    "a = torch.rand(2,4)\n",
    "\n",
    "b = torch.masked_select(input=a, mask=mask)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.选择非0tensor: torch.nonzeros(a)**\n",
    "\n",
    "**d.等分tensor: torch.split  & torch.chunck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.8825, 0.3812, 0.4115, 0.3213],\n",
      "        [0.2656, 0.0737, 0.5875, 0.0494]]),)\n",
      "(tensor([[0.8825, 0.3812],\n",
      "        [0.2656, 0.0737]]), tensor([[0.4115, 0.3213],\n",
      "        [0.5875, 0.0494]]))\n"
     ]
    }
   ],
   "source": [
    "# 在维度1上将a等分成大小为2的小块\n",
    "a_split = torch.split(a, 5, dim=0)\n",
    "print(a_split)\n",
    "\n",
    "# 在维度1上将a等分成2块\n",
    "a_chunk = torch.chunk(input=a, chunks=2, dim=1)\n",
    "print(a_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.通过tensor对象中的函数：**\n",
    "\n",
    "a.select(0,2)：获取a的第1行第3列位置的元素\n",
    "\n",
    "a.unfold(dim, size, step): 当step=1时返回a中维度为dim的所有大小为size的分片\n",
    "\n",
    "### 2.5.3 连接、聚合、压缩、变形...\n",
    "#### （1）tensor的连接：cat与stack\n",
    "两个常用的连接tensor的方式:\n",
    "\n",
    "torch.cat()拼接两个tensor， 但维度数量不变，如两个2*4的tensor在维度0上拼接，则生成一个4*4的新tensor,维度的数量仍然是2个；\n",
    "\n",
    "torch.stack()将两个2*4的tensor拼接之后会产生2*2*4的新tensor, 总的维度数目变成了3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([2, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,4)\n",
    "b = torch.randn(2,4)\n",
    "\n",
    "a_b_cat = torch.cat(tensors=(a,b), dim=0)\n",
    "a_b_stack = torch.stack(tensors=(a,b), dim=0)\n",
    "print(a_b_cat.shape)\n",
    "print(a_b_stack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）tensor的压缩与扩张：sequenze与unbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.rand(2,1,3,1)\n",
    "\n",
    "# 不传入维度参数，则默认去掉所有大小为1的维度，变成（2，3）大小\n",
    "a_squeeze = torch.squeeze(input=a)\n",
    "\n",
    "# 传入指定维度参数，则只去掉该维度,如下变成（2,3,1）大小\n",
    "a_squeeze = torch.squeeze(input=a, dim=1)\n",
    "\n",
    "# 去掉0维\n",
    "a_unbind = torch.unbind(a, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，可以用torch.unsqueeze()来扩张指定维度\n",
    "\n",
    "#### （3）tensor的聚合：ganther\n",
    "#### （4）tensor的转换：t, transpose\n",
    "t只适用于二维的Tensor的转置，即矩阵；而transpose是适合多维Tensor的维度转换."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3)\n",
    "a_t = torch.t(a)\n",
    "print(a_t.shape)\n",
    "\n",
    "a_ts = torch.transpose(a,1,0)\n",
    "print(a_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （5）tensor的变形：view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 12])\n",
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,3,4)\n",
    "print(a.shape)\n",
    "\n",
    "a = a.view(2, 12)\n",
    "print(a.shape)\n",
    "\n",
    "# 若参数为-1则表示自动计算该维度的大小\n",
    "a = a.view(-1, 12)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.4 数学操作\n",
    "数学操作的API可多呢~总共大概有90多,各下面几节依次分类介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）逐点操作\n",
    "逐点操作是对tensor中的每个值都进行相同的操作,如torch.abs(a), 是将tensor中的每个value都绝对值化，其他逐点操作大致罗列如下表。\n",
    "\n",
    "| 类型 | 功能 | 功能 | api |\n",
    "| --- | :--- | --- | --- |\n",
    "| 三角函数 | 余弦 | 余弦 | .cos(a) |\n",
    "|  |  | 双曲余弦 | .cosh(a) |\n",
    "|  |  | 反余弦 | .acos(a) |\n",
    "|  | 正弦 | 正弦 | .sin(a) |\n",
    "|  |  | 双曲正弦 | .sinh(a) |\n",
    "|  |  | 反正弦 | .asin(a) |\n",
    "|  | 正切 | 正切 | .tan(a) |\n",
    "|  |  | 双曲正切 | .tanh(a) |\n",
    "|  |  | 反正切 | .atan(a) |\n",
    "|  |  | 2个张量的反正切 | .atan2(a,b) |\n",
    "| 加减乘除 | 加法 |  | .add(a,b) |\n",
    "|  | 乘法 |  | .mul(a,b) #若张量*标量：逐点都乘以标量； 若张量*张量：对应元素乘 |\n",
    "|  | 除法 |  | .div(a,b) |\n",
    "|  | 先除后加 |  | .addcdiv(t, 0.5, t1, t2) # t+0.5(t1/t2) |\n",
    "|  | 先乘后加 |  | .addcmul(t, 0.5, t1, t2) # t+0.5(t1·t2)|\n",
    "|  | 除法余数 |  | .reminder(a,2)  # a%2 |\n",
    "| 指数对数 | 指数 |  | .exp(a) |\n",
    "|  | 自然对数 |  | .log(a) |\n",
    "| 幂与根 | 幂 |  | .pow(a,2).pow(a,a)  |\n",
    "|  | 平方根 |  | .sqrt(a) |\n",
    "|  | 平方根的倒数 |  | .rsqrt(a) |\n",
    "| 取整 | 向上取整 |  | .ceil(a) |\n",
    "|  | 向下取整 |  | .floor(a) |\n",
    "|  | 四舍五入 |  | .round(a) |\n",
    "|  | 截断 |  | tranc(a) |\n",
    "| 激活函数 | sigmoid |  | .sigmoid(a) |\n",
    "| 其他 | 返回分数 |  | .frac(a) |\n",
    "|  | 取负 |  | .neg(a) |\n",
    "|  | 取倒 |  | .recipocal(a) |\n",
    "|  | 取正负号 |  | .sign(a) |\n",
    "|  | 夹紧到某个区域 |  | .clamp(a, max, min) |\n",
    "|  | 线性差值 |  | .lerp() |\n",
    "\n",
    "\n",
    "#### （2）缩减操作\n",
    "缩减操作是指操作之后维度变小了，如求2*2的tensor每行的均值，得到的结果是2*1的新tensor。常用的缩减操作如下表罗列：\n",
    "\n",
    "| 基本运算 | 求均值 | .mean(a) # 求所有数值的均值，返回标量.mean(a, dim) # 求指定维度上的均值，返回数组 |\n",
    "| --- | --- | --- |\n",
    "|  | 求中位数 | .media(a)  #同上 |\n",
    "|  | 求积 | .prod(a) |\n",
    "|  | 求标准差 | .std(a) |\n",
    "|  | 求方差 | .var(a) |\n",
    "|  | 求和 | .sum(a) |\n",
    "| 累计运算 | 求累计和 | .cumsum(a, dim) |\n",
    "|  | 求累计积 | .cumprod(a,dim) |\n",
    "| 范数运算 | (a-b)的p范数 | dist(a, b, 3) |\n",
    "|  | a的p范数 | .norm(a,p) |\n",
    "\n",
    "\n",
    "#### （3）比较操作\n",
    "| 基本比较 | a == b ? | .eq(a,b) |\n",
    "| --- | --- | --- |\n",
    "|  | a>b ? | .gt(a,b) |\n",
    "|  | a <= b ? | .le(a,b) |\n",
    "|  | a < b ? | .lt(a,b) |\n",
    "|  | a != b ? | .ne(a,b) |\n",
    "\n",
    "\n",
    "除了以上这些基本的比较操作，pytorch还封装了一些基于比较的常用方法：\n",
    "\n",
    "| 取值 | 取指定维度第K个最小值 | .kthvalue(a,k) |\n",
    "| --- | --- | --- |\n",
    "|  | 取指定维度K个最大/小值 | .topk(a,k) |\n",
    "|  | 取最大值 | .max(a, dim) |\n",
    "|  | 取最小值 | .min(a, dim) |\n",
    "| 排序 | 排序， 返回排序后的tensor，与原index | .sort(a) |\n",
    "\n",
    "\n",
    "#### （4）线性代数操作\n",
    "\n",
    "| 点乘 | .dot(a,b) |\n",
    "| --- | --- |\n",
    "| 特征值、特征向量 | .eig(a,True) |\n",
    "| 对满秩矩阵计算最小二乘和最小范数 | .gels(B,A) |\n",
    "| OR分解 | gegrf(A) |\n",
    "| AX=B的解 | gesv(B,A) |\n",
    "| 取逆 | invese(a) |\n",
    "| 矩阵*矩阵 | .mm(x,y) |\n",
    "| 矩阵*向量 | .mv(x,y) |\n",
    "| 向量积 | .cross(a,b) |\n",
    "| 获取对角矩阵 | .diag(a) |\n",
    "| 返回上三角 | .tril() |\n",
    "| 返回下三角 | .triu() |\n",
    "\n",
    "有必要讲以下最常用的点乘、矩阵与向量乘、矩阵与矩阵乘的例子：\n",
    "\n",
    "- 首先看点乘，调用torch.dot()实现向量与向量的点积，即将两个向量对应位置的元素相乘，并求和，其输出是标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3])\n",
    "b = torch.Tensor([2,3,4])\n",
    "\n",
    "c = torch.dot(a,b)  # 1*2 + 2*3 + 3*3 = 20\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.mv是实现矩阵与向量相乘，比如矩阵a:\n",
    "$$\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   1 & 2 & 3 \\\\\n",
    "   4 & 5 & 6 \\\\\n",
    "   7 & 8 & 9\n",
    "  \\end{matrix}\n",
    "  \\right] \\tag{3}\n",
    "$$\n",
    "和向量b:\n",
    "$$\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   1 \\\\\n",
    "   2 \\\\\n",
    "   3\n",
    "  \\end{matrix}\n",
    "  \\right] \\tag{3}\n",
    "$$\n",
    "相乘："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14., 32., 50.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "b = torch.Tensor([1,2,3])\n",
    "\n",
    "c = torch.mv(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.5 序列化\n",
    "当我们想要把有价值的tensor保存在磁盘上以便下次读入使用时，就需要用到tensor的序列化操作。有两种方式可以操作，第一是使用torch 模块中的save/load功能，但是这样保存的结果只能适用于Pytorch读取，其他软件无法读取；因此第二种方法是使用HDF5格式， HDF5是一个接口，是很多软件支持的格式。\n",
    "#### （1）使用torch.save/load存取tensor\n",
    "\n",
    "保存："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "points = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])\n",
    "\n",
    "# 可以这样\n",
    "torch.save(points, path)\n",
    "\n",
    "# 也可以这样\n",
    "with open(path, 'w') as f:\n",
    "    torch.save(points, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 可以这样\n",
    "points = torch.load(path)\n",
    "\n",
    "# 也可以这样\n",
    "with open(path, 'r') as f:\n",
    "    points = torch.load(f)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）使用hdf5存取tensor\n",
    "先安装h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conda install h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存：\n",
    "注意，tensor是先转换成NumPy array， 再作为create_dataset的参数传入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import h5py\n",
    "\n",
    "f = h5py.File(path, 'w')\n",
    "dset = f.create_dataset('coords', data=points.numpy())\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "f = h5py.File(path, 'r')\n",
    "dset = f['coords']\n",
    "last_points = dset[1:]  # 返回的是NumPyArray\n",
    "last_points = torch.from_numpy(dset[1:])  # 要将NumPyArray转换成tensor\n",
    "f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.6 设备\n",
    "tensor可以放在CPU上计算，也可以放在GPU上计算，GPU能更快速，并行化地计算。有三种方式可以让tensor在设备之间转换。\n",
    "#### （1）将tensor放到GPU上的三种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）要注意的点\n",
    "\n",
    "- 当CPU上的tensor移动到了GPU上，原来的CPU类型也随之转变维GPU类型，如：torch.FloatTensor --> torch.cuda.FloatTensor\n",
    "- 当该tensor在已经在GPU上了，则在该Tensor上做的操作也都是在GPU上运算，运算后生成的新变量也是在GPU上的。\n",
    "- \n",
    "\n",
    "### 2.5.7 其他\n",
    "#### （1）判断是否为张量或storage\n",
    "\n",
    "- is_tensor\n",
    "- is_storage\n",
    "\n",
    "#### （2）设置打印\n",
    "\n",
    "- set_printoptions  （参考numpy）\n",
    "\n",
    "#### （3）设置/获取并行数\n",
    "\n",
    "- set_num_threads(n)\n",
    "- get_num_threads()\n",
    "\n",
    "#### （4）随机取样\n",
    "\n",
    "- 设置随机种子： manel_seed()\n",
    "- 设置初始种子： inital_seed()\n",
    "- 设置/获取随机状态：set_rng_state() / get_rng_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
