{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 利用PyTorch构造神经翻译机\n",
    "\n",
    "机器翻译其实是走得比较先的AI应用了，后来的许多自然语言生成的任务多少都在借鉴着机器翻译的一些研究与实践成果。如果你是做自然语言处理的，那么下面这篇论文是必读无疑的，关于sequence to sequence ,关于attention的知识也是必须知道。\n",
    "\n",
    "论文：Bahdanau, Neural Machine Translation By Jointly Language to Align and Translate\n",
    "\n",
    "本节就是针对以上论文中模型的一个简单实现。\n",
    "\n",
    "先导入本节代码需要的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 准备数据\n",
    "**原始数据**：\n",
    "\n",
    "机器翻译模型输入是语言A，输出是翻译后的结果语言B。在本项目的data/translation_data/路径下有一份平行语料，chinese1为中文，english1为英文，每行一个样本，两份数据的行之间是对应的翻译结果。\n",
    "\n",
    "**数据处理**：\n",
    "- （1）预处理数据，将文本从Unicode格式转换为ASCII格式，去除大部分标点符号，大写字母转换成小写字母，这些操作用normaliz_string函数进行封装。\n",
    "- （2）过滤数据，过滤掉一些不符合设定标注的句子，比如过滤掉长度大于30的句子, 用filter_pairs函数封装。\n",
    "- （3）构建词典，分别对两个语言的语料构建词典，即词对应数字编码，用类LangEmbed()来构建。\n",
    "\n",
    "下面看看代码具体的实现：\n",
    "\n",
    "1）预处理数据的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode2ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode2ascii(s.lower().strip()) # 小写、去前后空格、转ascii格式\n",
    "    s = re.sub(\"[.!?。！？]\", \"\\1\", s)  # 处理标点\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）过滤数据的函数, 为了简化，我们设置保留长度小于30的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    p = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) <= MAX_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "            p.append(pair)\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）构建词典的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LangEmbed():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "        self.n_word=2\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word)\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_word\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_word] = word\n",
    "            self.n_word += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）读入数据整个整个预处理流程, 返回两个语言的词典，与语料对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 10009 sentence paris\n",
      "36 sentence after filter\n",
      "number of chinese words 102\n",
      "number of english words 93\n",
      "[['\" 导弹 起 竖 完毕 \\x01', '\" missile erection is ready \\x01'], ['\" \" 发射 \\x01 \"', '\" fire \\x01 \"'], ['赶快 行动 吧 \\x01 回到 主页', 'get into action at once \\x01'], ['第一 , 它 是 轻 度 的 \\x01', 'it is mild \\x01'], ['这是 为什麽 \\x01', 'why \\x01']]\n",
      "0 : SOS\n",
      "1 : EOS\n",
      "2 : \"\n",
      "3 : 导弹\n",
      "4 : 起\n",
      "5 : 竖\n",
      "6 : 完毕\n",
      "7 : \u0001\n",
      "8 : 发射\n",
      "9 : 赶快\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    ch = open('data/translation_data/chinese1.txt', encoding='utf-8').readlines()\n",
    "    en = open('data/translation_data/english1.txt', encoding='utf-8').readlines()\n",
    "    pairs = [[normalize_string(ch[i]), normalize_string(en[i])] for i in range(len(ch)) ]\n",
    "    print(\"read %s sentence paris\" % (len(pairs)))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"%s sentence after filter\" % (len(pairs)))\n",
    "    \n",
    "    ch_lang = LangEmbed('ch')\n",
    "    en_lang = LangEmbed('en')\n",
    "    for pair in pairs:\n",
    "        ch_lang.add_sentence(pair[0])\n",
    "        en_lang.add_sentence(pair[1])\n",
    "    print('number of chinese words', ch_lang.n_word)\n",
    "    print('number of english words', en_lang.n_word)\n",
    "    \n",
    "    return ch_lang, en_lang, pairs\n",
    "\n",
    "ch_lang, en_lang, pairs = prepare_data()\n",
    "print(pairs[:5])\n",
    "for i in range(10):\n",
    "    print(i, \":\", ch_lang.index2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 构建模型\n",
    "分别构建encoder与decoder两部分\n",
    "\n",
    "1）encoder编码器的构建\n",
    "\n",
    "- 这里我们选择GRU单元，它有2个输入：当前步的词向量，上一步的隐向量；有1个输出：当前步的隐向量。使用nn.GRUCell来创建。\n",
    "\n",
    "- 输入GRU单元的是词嵌入向量，即输入的词one-hot向量先乘以词嵌入矩阵，得到词嵌入向量。注意这个词嵌入矩阵可以在创建时随机初始化，并通过训练来更新矩阵的值，也可以是直接使用外部的训练好的词嵌入矩阵，并选择训练时不更新该矩阵。这里选择的是第一种方式。词嵌入矩阵用nn.embedding来创建，第一个参数是输入的维度，第二个参数是输出的维度.\n",
    "\n",
    "- init_hidden函数是为了在外部调用初始化第一步时输入的hidden用的,这里初始化值为0，大小为（1，hidden_size）的矩阵, 第一维是batch_size，因为每一句的长度不一样，我们一次只输入一句话的一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRnn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.embedding(inputs)\n",
    "        hidden = self.gru(emb, hidden)  # gru有两个输入一个输出\n",
    "        return hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        if use_cuda:\n",
    "            return h0.cuda\n",
    "        else:\n",
    "            return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）decoder解码器的构建\n",
    "\n",
    "- 仍然使用gru单元，gru当前步的输入是上一步的输出y， 和上一步的隐向量，gru的输出是当前步的隐向量\n",
    "- gru输出的隐向量需要过一个线性计算和softmax， 使得输出为词典大小的向量，是当前步对每个词的预测概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRnn(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.embedding(inputs)\n",
    "        emb = F.relu(emb)\n",
    "        hidden = self.gru(emb, hidden)\n",
    "        output = F.log_softmax(self.out(hidden))\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        if use_cuda:\n",
    "            return h0.cuda\n",
    "        else:\n",
    "            return h0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 训练\n",
    "\n",
    "1）首先我们构建一个函数，它的功能是对输入的一个样本进行前向后向并输出损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "def train(inputs, targets, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_len=MAX_LENGTH):\n",
    "    # 1.前期工作==================================================\n",
    "    encoder_hidden = encoder.init_hidden() # 初始化encoder的gru参数\n",
    "    \n",
    "    encoder_optimizer.zero_grad()  # encoder梯度置零\n",
    "    decoder_optimizer.zero_grad()  # decoder梯度置零\n",
    "    \n",
    "    input_length = inputs.size()[0]   # 输入句子的词数\n",
    "    target_length = targets.size()[0]  # 输出句子的词数\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    # 2.句子进行encoder端的计算==================================\n",
    "    # 2.1初始化encoder端的输出：输出每步的隐向量，n个词即n步\n",
    "    encoder_outputs = torch.zeros(max_len, encoder.hidden_size)\n",
    "    if use_cuda:\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "    \n",
    "    # 2.2将一句话中的每个词依次输入enocder中，前一步的隐向量是当前步的输入\n",
    "    for ei in range(input_length):\n",
    "        encoder_hidden = encoder(inputs[ei], encoder_hidden)\n",
    "     \n",
    "    \n",
    "    # 3.句子进行decoder端的计算==================================\n",
    "    # 3.1 设置第一步的输入x,即预测第一个词时的输入为句子起始标识符SOS_token\n",
    "    decoder_input = torch.LongTensor([SOS_token])\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_inputs.cuda()\n",
    "        \n",
    "    decoder_hidden = encoder_hidden # encoder端的最后一步输出隐向量，是decoder端的第一步的输入隐向量\n",
    "        \n",
    "    # 3.2设置是否设置teacher_forcing\n",
    "    if random.random() < teacher_forcing_ratio:\n",
    "        use_teacher_forcing = True\n",
    "    else:\n",
    "        use_teacher_forcing = False\n",
    "        \n",
    "    # 3.3 进行decoder端的每步循环\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length): # 循环每步\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) # 经过decoder计算\n",
    "            loss += criterion(decoder_output, targets[di])  # 直接用decoder的输出去求损失\n",
    "            decoder_input = targets[di] # decoder的输出直接作为encoder的输入\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)# 经过decoder计算（与上不变）\n",
    "            topv, topi = decoder_output.data.topk(1)  # 获得概率最大的值，及其索引\n",
    "            ni = topi[0][0]  # 获取其在词典中对应的索引，因为decoder_output是二维（1，output_size)\n",
    "            decoder_input = torch.LongTensor([ni]) # 下一步的input，是上一步的output的概率最大词的索引\n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "            loss += criterion(decoder_output, targets[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "      \n",
    "    \n",
    "    # 3.4 反向传播与更新参数=======================================\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解释一下topk(i)，是找出tensor中值最大的那个数，并返回它的值和索引。所以上面代码上topv, topi分别指decoder输出向量中的最大值与其所在的索引位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[4.]]), tensor([[1]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[2,4,3,1]])\n",
    "print(a.topk(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解释一下teacher_forcing（重要）\n",
    "\n",
    "设置了一定的比例的数据是进行teacher_forcing，一部分不进行teacher_forcing。两者的区别是，a.前者的当前步骤的input是真实目标句子中的前一个词（相当于是用真实的结果当老师去提点了），后者的当前步骤的input是上一步预测出概率最大的词。 b.后者设置了循环终止条件。\n",
    "\n",
    "- 解释一下损失\n",
    "\n",
    "函数最终返回的是总损失除以目标句子的词数。\n",
    "\n",
    "2）接着构建一个迭代处理所有句子的训练函数\n",
    "\n",
    "先提前设置一个有用的功能函数：index_from_pairs(),即将输入的句子对，根据词典将文字转换成索引的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2index(lang, sentence):\n",
    "    return [lang.word2index[w] for w in sentence.split()]\n",
    "\n",
    "def index_from_sentence(lang, sentence):\n",
    "    index = sentence2index(lang, sentence)\n",
    "    index.append(EOS_token) # 追加一个句子结尾符\n",
    "    re = torch.LongTensor(index).view(-1, 1)  # 增加一个维度\n",
    "    if use_cuda:\n",
    "        re = re.cuda()\n",
    "        \n",
    "    return re\n",
    "\n",
    "def index_from_pairs(pair):\n",
    "    inputs = index_from_sentence(ch_lang, pair[0])\n",
    "    targets = index_from_sentence(en_lang, pair[1])\n",
    "    return [inputs, targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来写完整训练的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:1, idx:0, average loss:0.0093\n",
      "iteration:2, idx:0, average loss:0.1971\n",
      "iteration:3, idx:0, average loss:0.2324\n",
      "iteration:4, idx:0, average loss:0.2300\n",
      "iteration:5, idx:0, average loss:0.2226\n",
      "iteration:6, idx:0, average loss:0.2226\n",
      "iteration:7, idx:0, average loss:0.2126\n",
      "iteration:8, idx:0, average loss:0.2066\n",
      "iteration:9, idx:0, average loss:0.2026\n",
      "iteration:10, idx:0, average loss:0.1983\n"
     ]
    }
   ],
   "source": [
    "def train_iters( n_iter, print_every=500, learning_rate=0.01, hidden_size=256):\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    # 模型、优化器、损失\n",
    "    encoder = EncoderRnn(ch_lang.n_word, hidden_size)\n",
    "    decoder = DecoderRnn(hidden_size, en_lang.n_word)\n",
    "    if use_cuda:\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "    \n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # 循环迭代，每次迭代为一个样本\n",
    "    for i in range(1, n_iter+1):\n",
    "        random.shuffle(pairs)\n",
    "        training_pairs = [index_from_pairs(pair) for pair in pairs]  # 所有句子转换成索引形式\n",
    "        \n",
    "        for idx, training_pair in enumerate(training_pairs):\n",
    "            input_index = training_pair[0]\n",
    "            target_index = training_pair[1]\n",
    "            loss = train(input_index, target_index, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) # 调用train\n",
    "            print_loss_total += loss\n",
    "            \n",
    "            if idx % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('iteration:%s, idx:%d, average loss:%.4f' % (i, idx, print_loss_avg))\n",
    "                \n",
    "\n",
    "train_iters(n_iter=10)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: \" 这就是 我们 的 底线 \u0001\n",
      "target: this is our bottom line \u0001\n",
      "predict: raised raised always always always always always always always always\n",
      "\n",
      "source: 这 又是 为什麽 \u0001\n",
      "target: again , why is that \u0001\n",
      "predict: raised raised always always always always always always always always\n",
      "\n",
      "source: 我爱你中国\u0001\n",
      "target: i love you ,china\u0001\n",
      "predict: ready raised always always always always always always always always\n",
      "\n",
      "source: 这 叫 什 麽 \u0001\n",
      "target: what is that called \u0001\n",
      "predict: raised raised always always always always always always always always\n",
      "\n",
      "source: 素质 是 高 还是 低 \u0001\n",
      "target: are they competent \u0001\n",
      "predict: raised raised raised always always always always always always always\n",
      "\n",
      "source: 素质 是 高 还是 低 \u0001\n",
      "target: are they competent \u0001\n",
      "predict: raised raised raised always always always always always always always\n",
      "\n",
      "source: 是否 先进 \u0001\n",
      "target: are they advanced weapons \u0001\n",
      "predict: raised raised raised always always always always always always always\n",
      "\n",
      "source: 一 只 只 手 高高 举起 \u0001\n",
      "target: a hand was raised high \u0001\n",
      "predict: kosovo raised always always always always always always always always\n",
      "\n",
      "source: 再说 科索沃 \u0001\n",
      "target: or take kosovo \u0001\n",
      "predict: kosovo raised always always always always always always always always\n",
      "\n",
      "source: \" \" 发射 \u0001 \"\n",
      "target: \" fire \u0001 \"\n",
      "predict: raised raised always always always always always always always always\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    inputs = index_from_sentence(ch_lang, sentence)\n",
    "    inputs_length = inputs.size()[0]\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "    \n",
    "    for ei in range(inputs_length):\n",
    "        encoder_hidden = encoder(inputs[ei], encoder_hidden)\n",
    "        \n",
    "    decoder_inputs = torch.LongTensor([SOS_token])\n",
    "    decoder_inputs = decoder_inputs.cuda() if use_cuda else decoder_inputs\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_words = []\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_inputs, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0].item()\n",
    "        \n",
    "        if ni == EOS_token:\n",
    "            decoder_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoder_words.append(en_lang.index2word[ni])\n",
    "        \n",
    "        decoder_input = torch.LongTensor([[ni]])\n",
    "        decoder_inputs = decoder_inputs.cuda() if use_cuda else decoder_inputs\n",
    "        \n",
    "    return decoder_words\n",
    "        \n",
    "def evaluat_randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('source:', pair[0])\n",
    "        print('target:', pair[1])\n",
    "        output_word = evaluate(encoder, decoder, pair[0])\n",
    "        output_word = ' '.join(output_word)\n",
    "        print('predict:', output_word)\n",
    "        print()\n",
    "\n",
    "encoder = EncoderRnn(ch_lang.n_word, 256)\n",
    "decoder = DecoderRnn(256, en_lang.n_word)\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "evaluat_randomly(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上结果还不成形，因为迭代次数和语料的问题，大家可以拿更好的语料，并将迭代次数增多进行测试\n",
    "\n",
    "### 7.1.5 Attention机制\n",
    "以上完成了一个翻译机的全部过程，但这是非常基础的网络结构，通常情况下我们会加入attention机制，那么模型结构encoder与decoder需要稍作改写。\n",
    "\n",
    "encoder端的改写：主要是将gru的outputs也作为输出返回，这个outputs会用于decoder端的attention计算相似分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRnn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)  # 直接调用GRU\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.embedding(inputs).view(1,1,-1)  # 转变了维度\n",
    "        output, hidden = self.gru(emb, hidden)  # 有两个输出\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1,1, self.hidden_size)  \n",
    "        if use_cuda:\n",
    "            return h0.cuda\n",
    "        else:\n",
    "            return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder端的改变比较大："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRnn(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRnn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.embedding(inputs).view(1,1,-1)\n",
    "        emb = self.dropout(emb)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedding[0], hidden[0]), 1)),dim=1)  #注意力计算\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.inzqueeze(0))  # 注意力抓取\n",
    "        \n",
    "        output = torch.cat((emb[0],attn_applied[0]), 1) \n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        \n",
    "        output = F.relu(out)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(1, 1, self.hidden_size)\n",
    "        if use_cuda:\n",
    "            return h0.cuda\n",
    "        else:\n",
    "            return h0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
