{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 RNN\n",
    "\n",
    "6.2节的图像分类用了CNN， 本节介绍RNN的PyTorch实现。\n",
    "\n",
    "相关资料：\n",
    "- 可以参考我的博客：https://blog.csdn.net/sinat_33761963/article/details/53521185\n",
    "- 我的另一篇博客：https://blog.csdn.net/sinat_33761963/article/details/80195636\n",
    "- 网上的课程\n",
    "\n",
    "很多复杂的模型都是基于RNN的结构，因此本文先介绍最基础的RNN，为以后的学习做好准备。\n",
    "\n",
    "### 6.5.1准备数据\n",
    "为了简化，这里先捏造一批数据，注意打印出来看看最终z,y的数据形式，大小为（1，1，100），第一个维度是batch_size\n",
    "\n",
    "- unsqueeze()这个方法关注下，会经常用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000],\n",
      "         [ 0.6428],\n",
      "         [ 0.9848],\n",
      "         [ 0.8660],\n",
      "         [ 0.3420],\n",
      "         [-0.3420],\n",
      "         [-0.8660],\n",
      "         [-0.9848],\n",
      "         [-0.6428],\n",
      "         [ 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "steps = np.linspace(0, np.pi*2, 10, dtype=np.float32)\n",
    "x = np.sin(steps)\n",
    "y = np.cos(steps)\n",
    "\n",
    "x = torch.from_numpy(x).unsqueeze(0).unsqueeze(-1)\n",
    "y = torch.from_numpy(y).unsqueeze(0).unsqueeze(-1)\n",
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 构建LSTM网路结构\n",
    "\n",
    "lstm是RNN的其中一中结构，也是目前业界非常常用的一种结构。\n",
    "\n",
    "- nn.LSTM可以直接创建一个lstm结构，其中参数input_size是输入向量的维度， hidden_size是输出隐向量的维度，batch_first=True表示输入与输出的第一维是batch_size。\n",
    "- lstm中每步都有三个输入输出，输入：当前步的输入x， 上一步的输出c和h；输出：隐向量h, 这一步的c和h,这样就可以理解forward中的第一行代码的参数。\n",
    "- def init_hidden是供外部调用的一个参数初始化的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=20,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, x, h_state, c_state):\n",
    "        r_out, (h_state, c_state) = self.lstm(x, (h_state, c_state))\n",
    "        outputs = self.out(r_out[0,:]).unsqueeze(0)\n",
    "        return outputs, h_state, c_state\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h_state = torch.randn(1,1,20)\n",
    "        c_state = torch.randn(1,1,20)\n",
    "        return h_state, c_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.546\n",
      "loss:0.536\n",
      "loss:0.536\n",
      "loss:0.536\n",
      "loss:0.536\n",
      "loss:0.536\n",
      "loss:0.535\n",
      "loss:0.535\n",
      "loss:0.535\n",
      "loss:0.535\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(INPUT_SIZE=1)\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=0.001)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "h_state, c_state = lstm.init_hidden()\n",
    "\n",
    "for step in range(100):\n",
    "    prediction, h_state, c_state = lstm(x, h_state, c_state)\n",
    "    h_state = h_state.data\n",
    "    c_state = c_state.data\n",
    "    \n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(\"loss:%.3f\" % (loss.data))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "1.事先学习RNN家族的理论知识\n",
    "2.明白如何构建RNN，Pytorch提供了RNN,LSTM,GRU三种封装好的结构接口，也提供了RNNCell, LSTMCell, GRUCell三种循环网络的单元结构，官网连接：https://pytorch.org/docs/stable/nn.html#recurrent-layers\n",
    "3.做循环神经网络，输入输出参数的维度，数据在整个过程中的形态都需要搞清楚。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
